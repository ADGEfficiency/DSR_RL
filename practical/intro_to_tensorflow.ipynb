{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an introduction in how to use the Tensorflow framework.\n",
    "\n",
    "We will cover the components needed for a simple feedfoward neural network.\n",
    "\n",
    "Key concepts & Tensorflow paradigms covered:\n",
    "- Placeholders\n",
    "- Variable\n",
    "- Initializers\n",
    "- Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  we use a set of tuples to define the network structure\n",
    "input_shape = (4,)  #  the length of the input numpy array\n",
    "input_nodes = (6,)  #  the number of nodes in the input layer\n",
    "hidden_nodes = (8,)  \n",
    "output_shape = (4,)  #  num. nodes in output layer and the length of the output numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Tensorflow paradigm is the **Placeholder**.\n",
    "\n",
    "Tensorflow uses placeholders to feed data into the network.\n",
    "\n",
    "The placeholder is fed using numpy arrays.\n",
    "\n",
    "The first dimension is the number of samples in the batch - we use None to be able to input any batch size we want.\n",
    "\n",
    "The second dimension is the shape of one sample - i.e. the length of the input numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_input = tf.placeholder(tf.float32, shape=(None, *input_shape), name='network_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two Tensorflow paradigms are the **Variable** and **Initializer**.  \n",
    "\n",
    "Variable objects are used to hold tensors of variable that tensorflow can change.  Both weights and biases are tf.Variables.\n",
    "\n",
    "We also need to tell Tensorflow what initial values we want our varibles to be.  \n",
    "\n",
    "To do the initialization we pass in the tf.random_normal initializer.  The shape of the Variable tensor is specified in the intializer.\n",
    "\n",
    "The * operation is used to unpack the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_weights = tf.Variable(tf.random_normal(shape=(*input_shape, *input_nodes), name='input_weights'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same pattern for the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_bias = tf.Variable(tf.zeros(shape=(*input_nodes,)), name='input_bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can form the input layer using matrix multiplication between the input & weights, then add the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_activation = tf.add(tf.matmul(network_input, input_weights), input_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can form the layer by squeezing the output through a rectified linear unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.nn.relu(pre_activation, name='input_layer_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a hidden layer using the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_w = tf.Variable(tf.random_normal((*input_nodes, *hidden_nodes)), name='hidden_weights')\n",
    "h_b = tf.Variable(tf.zeros((*hidden_nodes, ), name='hidden_bias'))\n",
    "hidden_layer = tf.nn.relu(tf.add(tf.matmul(input_layer, h_w), h_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer has no activation function (aka a linear activation function).  This allows the network to output negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o_w = tf.Variable(tf.random_normal((*hidden_nodes, *output_shape)), name='output_weights')\n",
    "o_b = tf.Variable(tf.zeros((*output_shape, ), name='output_bias'))\n",
    "output = tf.add(tf.matmul(hidden_layer, o_w), o_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the code above allows us to make predictions with our network - i.e. we can do forward passes across the network.\n",
    "\n",
    "Below we will setup the code for training.\n",
    "\n",
    "We first need another placeholder.  This will serve as the target value for the network (aka y_train) - what our network should be outputting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = tf.placeholder(tf.float32, shape=(None, *output_shape), name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(target, output, scope='loss_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an optimizer to do the heavy lifting of training the network.  \n",
    "\n",
    "Here we use the Adam optimizer.  Note that the input here of learning rate is one of the most important in training any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a Tensorflow operation to actually do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the machinery to do forward passes and to back propagate error is in place.  \n",
    "\n",
    "Add in a few Tensorflow summary operations to track what is going on in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.tensor_summary('input', network_input)\n",
    "tf.summary.histogram('input_weights', input_weights)\n",
    "tf.summary.histogram('output_bias', o_b)\n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Tensorflow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing our model needs is data.  The function below generates training data for a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generates training data for our network.\n",
    "\n",
    "    args\n",
    "        num_samples (int)\n",
    "\n",
    "    returns\n",
    "        net_in (np.array)  fed into the network (aka features or x)\n",
    "        net_out (np.array)  aka target or y - the value we are trying to approx.\n",
    "    \"\"\"\n",
    "    net_in = np.random.rand(num_samples, *input_shape)\n",
    "    net_out = net_in + 10\n",
    "\n",
    "    return net_in, net_out\n",
    "\n",
    "#  run the function to get data to train with\n",
    "inputs, targets = generate_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce another key Tensorflow paradigm - the Session.\n",
    "\n",
    "You can think of a Session as one instance of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 109.13784790039062\n",
      "step 1 loss 108.73883056640625\n",
      "step 2 loss 108.33975219726562\n",
      "step 3 loss 107.94058227539062\n",
      "step 4 loss 107.54119873046875\n",
      "step 5 loss 107.14191436767578\n",
      "step 6 loss 106.74327850341797\n",
      "step 7 loss 106.34396362304688\n",
      "step 8 loss 105.94388580322266\n",
      "step 9 loss 105.5434341430664\n",
      "step 10 loss 105.14512634277344\n",
      "step 11 loss 104.74711608886719\n",
      "step 12 loss 104.3487319946289\n",
      "step 13 loss 103.94924926757812\n",
      "step 14 loss 103.54931640625\n",
      "step 15 loss 103.14877319335938\n",
      "step 16 loss 102.74767303466797\n",
      "step 17 loss 102.34693145751953\n",
      "step 18 loss 101.94611358642578\n",
      "step 19 loss 101.54527282714844\n",
      "step 20 loss 101.1436996459961\n",
      "step 21 loss 100.74148559570312\n",
      "step 22 loss 100.33831024169922\n",
      "step 23 loss 99.93470001220703\n",
      "step 24 loss 99.53063201904297\n",
      "step 25 loss 99.12427520751953\n",
      "step 26 loss 98.71720886230469\n",
      "step 27 loss 98.30912017822266\n",
      "step 28 loss 97.89996337890625\n",
      "step 29 loss 97.49026489257812\n",
      "step 30 loss 97.0799331665039\n",
      "step 31 loss 96.66816711425781\n",
      "step 32 loss 96.255859375\n",
      "step 33 loss 95.84242248535156\n",
      "step 34 loss 95.42777252197266\n",
      "step 35 loss 95.01242065429688\n",
      "step 36 loss 94.596435546875\n",
      "step 37 loss 94.1796646118164\n",
      "step 38 loss 93.76187133789062\n",
      "step 39 loss 93.34335327148438\n",
      "step 40 loss 92.92390441894531\n",
      "step 41 loss 92.5033950805664\n",
      "step 42 loss 92.08195495605469\n",
      "step 43 loss 91.66006469726562\n",
      "step 44 loss 91.2376937866211\n",
      "step 45 loss 90.81416320800781\n",
      "step 46 loss 90.38988494873047\n",
      "step 47 loss 89.96353149414062\n",
      "step 48 loss 89.53608703613281\n",
      "step 49 loss 89.1069564819336\n",
      "step 50 loss 88.67692565917969\n",
      "step 51 loss 88.24580383300781\n",
      "step 52 loss 87.8133773803711\n",
      "step 53 loss 87.37970733642578\n",
      "step 54 loss 86.94390869140625\n",
      "step 55 loss 86.5066146850586\n",
      "step 56 loss 86.06808471679688\n",
      "step 57 loss 85.62948608398438\n",
      "step 58 loss 85.18989562988281\n",
      "step 59 loss 84.74979400634766\n",
      "step 60 loss 84.3095932006836\n",
      "step 61 loss 83.86906433105469\n",
      "step 62 loss 83.42807006835938\n",
      "step 63 loss 82.986083984375\n",
      "step 64 loss 82.54376220703125\n",
      "step 65 loss 82.10066223144531\n",
      "step 66 loss 81.65751647949219\n",
      "step 67 loss 81.21430969238281\n",
      "step 68 loss 80.77096557617188\n",
      "step 69 loss 80.32752227783203\n",
      "step 70 loss 79.8846664428711\n",
      "step 71 loss 79.44182586669922\n",
      "step 72 loss 78.99915313720703\n",
      "step 73 loss 78.55662536621094\n",
      "step 74 loss 78.11431884765625\n",
      "step 75 loss 77.67229461669922\n",
      "step 76 loss 77.23033905029297\n",
      "step 77 loss 76.78851318359375\n",
      "step 78 loss 76.3468246459961\n",
      "step 79 loss 75.9050521850586\n",
      "step 80 loss 75.4634017944336\n",
      "step 81 loss 75.02201843261719\n",
      "step 82 loss 74.58091735839844\n",
      "step 83 loss 74.14000701904297\n",
      "step 84 loss 73.69961547851562\n",
      "step 85 loss 73.25949096679688\n",
      "step 86 loss 72.8196029663086\n",
      "step 87 loss 72.37991333007812\n",
      "step 88 loss 71.94049072265625\n",
      "step 89 loss 71.50114440917969\n",
      "step 90 loss 71.06201934814453\n",
      "step 91 loss 70.6231918334961\n",
      "step 92 loss 70.18446350097656\n",
      "step 93 loss 69.7459487915039\n",
      "step 94 loss 69.3072509765625\n",
      "step 95 loss 68.86898803710938\n",
      "step 96 loss 68.43148040771484\n",
      "step 97 loss 67.99447631835938\n",
      "step 98 loss 67.55835723876953\n",
      "step 99 loss 67.12271118164062\n",
      "[[ 1.6729844   3.7445893   6.5544443  -0.8732805 ]\n",
      " [ 0.7887651   2.2574878   2.6435714   0.327914  ]\n",
      " [ 2.8445277   2.136828    6.321263    0.9391912 ]\n",
      " [ 2.114752    1.8935577   5.1280084   0.9223285 ]\n",
      " [ 1.7286227   2.9843044   5.830824   -0.10626099]]\n",
      "[[10.19102793 10.93693273 10.1836215  10.3905504 ]\n",
      " [10.14743048 10.25944746 10.70091422 10.70787792]\n",
      " [10.65931137 10.4394587  10.25072999 10.40100644]\n",
      " [10.93399807 10.19138263 10.66125908 10.13752872]\n",
      " [10.01809728 10.81450378 10.19835436 10.57462968]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    #  first a necessary bit of Tensorflow boiler plate - initializing variables\n",
    "    #  we do this operation using the Session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #  we create a FileWriter object to write out summarys to a file for Tensorboard to read\n",
    "    writer = tf.summary.FileWriter('./logs', graph=sess.graph)\n",
    "    \n",
    "    #  now the training loop\n",
    "    #  we are not splitting our training data into batches\n",
    "    \n",
    "    for train_step in range(100):\n",
    "        #  now we run the tensorflow graph\n",
    "        #  the graph is run by calling the .run method on the session\n",
    "        #  the run method takes two inputs:\n",
    "        #   fetches = the tf operations to run\n",
    "        #   feed_dict = values for the placeholders\n",
    "\n",
    "        #  here we fetch two operations\n",
    "        #   train_op - the operation to train the network\n",
    "        #   summary - the summary operations for the graph\n",
    "        fetches = [loss, train_op, merged]\n",
    "\n",
    "        #  the feed_dict is a dictionary with\n",
    "        #   keys = the placeholders\n",
    "        #   values = the numpy arrays\n",
    "        #   note that we feed in multiple samples\n",
    "        feed_dict = {network_input: inputs,\n",
    "                     target: targets}\n",
    "        #  finally we run the session using the fetches and feed_dict\n",
    "        loss_value, _, summary = sess.run(fetches, feed_dict)\n",
    "        #  the operation to add the summary to the tensorboard output file\n",
    "        writer.add_summary(summary, train_step)\n",
    "        print('step {} loss {}'.format(train_step, loss_value))        \n",
    "        \n",
    "    #  now training is done\n",
    "    #  generate a test set \n",
    "    test_in, test_out = generate_data(5)\n",
    "\n",
    "    #  here we get predictions from our network - we don't train\n",
    "    pred = sess.run(output, {network_input: test_in})\n",
    "    print(pred)\n",
    "    print(test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run view the data plotted to tensorboard\n",
    "\n",
    "```\n",
    "$ cd dsr_rl/practical/generic_lessons\n",
    "$ tensorboard --logdir='./logs'\n",
    "```\n",
    "\n",
    "Now go open a browser and go to\n",
    "`http://localhost:6006/`\n",
    "\n",
    "And you should be able to see the tensorboard server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
