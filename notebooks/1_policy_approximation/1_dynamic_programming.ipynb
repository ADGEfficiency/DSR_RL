{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to demonstrate dynamic programming as a method for policy approximation.\n",
    "\n",
    "Aim is for you to understand\n",
    "- concept of what we need to define a Markov Decision Process (state transitons, reward transitions)\n",
    "- how we use the Bellman equation to bootstrap our value function approximation\n",
    "\n",
    "- how dynamic programming can learn value functions without taking actions]\n",
    "- how dynamic programming needs the environment model to work\n",
    "\n",
    "http://bozskyfilip.blogspot.co.uk/2009/04/temporal-differential-learning-policy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = np.array(range(0,7))\n",
    "terminal_states = [0,6]\n",
    "action_space = np.array(['left', 'right'])\n",
    "reward = [ 1 if s == 6 else 0 for s in state_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  loading state transition probabilities from csvs\n",
    "state_transitions = np.array[1, ]\n",
    "state_transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1.,  1.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, state in enumerate(state_space[:-2]):\n",
    "    state_transitions[0][i] = 1\n",
    "    state_transitions[1][i+1] = 1\n",
    "state_transitions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the state transition probabilities is a lookup table\n",
    "# note that rows = actions, columns = states\n",
    "    \n",
    "#        's1' 's2' 's3' 's4' 's5'   \n",
    "# 'left'\n",
    "# 'right'\n",
    "# 'up'\n",
    "# 'down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are the probabilities for state s1\n",
    "# the probability of action 'right' taking us to 's2' is 1\n",
    "# the probability of action 'up' taking us to 's1' is 1\n",
    "# i.e. P(s'|s,a)\n",
    "\n",
    "state_transitions['s1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  we can now do our reward transition probabilities\n",
    "reward_functions = {state:np.full((len(state_space)),-1) for state in state_space}\n",
    "\n",
    "#  these work in a similar way to our state transitions, except they are not conditioned upon actions\n",
    "#  only conditions upon state, next_state\n",
    "#  i.e R(s, s')\n",
    "#  this is OK for our problem - a more formal problem would have R(s, a, s')\n",
    "\n",
    "#  reward of all state->next_state transitons is -1\n",
    "reward_functions['s4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  now we manually fill in the rewards for our special states\n",
    "reward_functions['s4'][4] = 10\n",
    "reward_functions['s4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  's5' is a special absorbing state with no rewards and transition prob of 1 to itself\n",
    "reward_functions['s5'] = np.full((len(state_space)),0)\n",
    "reward_functions['s5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  we need a policy to approximate - lets use a random policy\n",
    "\n",
    "def random_policy(state, action_space):\n",
    "    \"\"\"\n",
    "    A random policy\n",
    "    \n",
    "    Args\n",
    "        state        : str  : the name of the current state\n",
    "        action_space : list : a list of all possible actions\n",
    "        \n",
    "    Returns\n",
    "        action       : str  : the name of the selected action\n",
    "        p_dist       : list : a probability distribution over the action space\n",
    "    \"\"\"\n",
    "    action = np.random.choice(action_space)\n",
    "    p_dist = np.full(len(action_space),  1/len(action_space))\n",
    "    assert np.sum(p_dist) == 1\n",
    "    return action, p_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dynamic_Programmer(object):\n",
    "    \"\"\"\n",
    "    Policy approximator based on dynamic programming\n",
    "    \n",
    "    Args\n",
    "        state_space  : list : a list of all possible states\n",
    "        action_space : list : a list of all possible actions\n",
    "        state_transitions : array : an array of the environment state transiton probabilities\n",
    "                                    dimensions = [actions, states]\n",
    "        reward_functions : array : an array of the rewards.  conditioned only on s, s' \n",
    "                                   dimensions = [state, state]\n",
    "        policy : function : a function that maps state to action & a probability distribution over actions  \n",
    "        discount : float : the discount factor (gamma)\n",
    "        verbose : int : controls printing\n",
    "        \n",
    "    Methods\n",
    "        policy transitions : creates the a probability distribution over states\n",
    "                             conditioned on both environment & policy\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 state_transitions, \n",
    "                 reward_functions,\n",
    "                 policy,\n",
    "                 discount,\n",
    "                 verbose=1):    \n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.state_transitions = state_transitions\n",
    "        self.reward_functions = reward_functions\n",
    "        self.policy = policy\n",
    "        self.discount = discount\n",
    "        self.verbose = verbose\n",
    "    \n",
    "        self.v_table = np.full(len(state_space), 0.0)\n",
    "\n",
    "    def policy_transitions(self, state):\n",
    "        \"\"\"\n",
    "        For a given state, returns a distribution over\n",
    "        next states (not over actions!)\n",
    "        \n",
    "        conditioned on both the environment (self.state_transitions)\n",
    "        and the policy (act_p_dist)\n",
    "        \n",
    "        Args\n",
    "            state : str : the current state\n",
    "            \n",
    "        Returns\n",
    "            all_probs : array : probability distribution over states\n",
    "        \"\"\"\n",
    "        state_idx = np.argwhere(state==self.state_space).flatten()\n",
    "        #  our policy gives us a distribution over actions\n",
    "        _, action_p_dist = self.policy(state, self.action_space)\n",
    "        \n",
    "        #  we can now use our perfect environment model to \n",
    "        #  develop the state transition probabilities\n",
    "        \n",
    "        for next_state in self.state_space:    \n",
    "            all_probs = np.full(len(state_space),0)\n",
    "\n",
    "            for act_idx, act_prob in enumerate(action_p_dist):   \n",
    "                \n",
    "                state_probs = self.state_transitions[state][act_idx].flatten()\n",
    "                probs = act_prob * state_probs\n",
    "                all_probs = np.add(all_probs, probs)\n",
    "\n",
    "        print('policy transitions are {} for state {}'.format(all_probs, state))\n",
    "        assert np.sum(all_probs) == 1        \n",
    "        return all_probs\n",
    "    \n",
    "    def backup_state(self, state, probabilities): \n",
    "        \"\"\"\n",
    "        Backs up a given state & probability distribution\n",
    "        \n",
    "        Args\n",
    "            state : str : the current state\n",
    "            probabilities : array : probability distribution over states\n",
    "            \n",
    "        Returns\n",
    "            self.v_table : array : the value function lookup table\n",
    "        \"\"\"\n",
    "        rewards = self.reward_functions[state]\n",
    "        discounted_values = self.discount * self.v_table  # note the recursive use of v.table!\n",
    "        bellman_equations = np.add(rewards, discounted_values)\n",
    "        \n",
    "        print('probabilities {}'.format(probabilities))\n",
    "        print('bellman equations {}'.format(bellman_equations))\n",
    "        \n",
    "        state_value = np.sum(np.multiply(probabilities, bellman_equations))\n",
    "        print('state value of state {} is {}'.format(state, state_value))\n",
    "        \n",
    "        state_idx = np.argwhere(state==self.state_space).flatten()\n",
    "        self.v_table[state_idx] = state_value\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('backing up state {}'.format(state))\n",
    "            print('probabilities are {}'.format(probabilities))\n",
    "            print('rewards are {}'.format(rewards))\n",
    "            print('discounted_values are {}'.format(discounted_values))  \n",
    "            print('bellman equations are {}'.format(bellman_equations))\n",
    "            print('state value is {}'.format(state_value))\n",
    "            print('value function table {}'.format(self.v_table))\n",
    "        return self.v_table\n",
    "    \n",
    "    def backup_all_states(self):\n",
    "        \"\"\"\n",
    "        Runs a backup over the entire state space\n",
    "        \n",
    "        Returns\n",
    "            v_table : array : the value function lookup table\n",
    "        \"\"\"\n",
    "        for state in self.state_space:\n",
    "            probabilities = self.policy_transitions(state)\n",
    "            self.v_table = self.backup_state(state, probabilities)\n",
    "        return self.v_table  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-1. -1. -1. -1. -1.]\n",
      "state value of state s1 is -1.0\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-1.9 -1.  -1.  -1.  -1. ]\n",
      "state value of state s2 is -1.225\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-1.9    -2.1025 -1.     -1.     -1.    ]\n",
      "state value of state s3 is -1.225\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -1.9     -2.1025  -2.1025  -1.      10.    ]\n",
      "state value of state s4 is 1.19875\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-0.9      -1.1025   -1.1025    1.078875  0.      ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-1.      -1.225   -1.225    1.19875  0.     ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-1.9      -2.1025   -2.1025    0.078875 -1.      ]\n",
      "state value of state s1 is -2.0012499999999998\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-2.801125 -2.1025   -2.1025    0.078875 -1.      ]\n",
      "state value of state s2 is -1.7318125\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-2.801125   -2.55863125 -2.1025      0.078875   -1.        ]\n",
      "state value of state s3 is -1.18646875\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -2.801125    -2.55863125  -2.06782187   0.078875    10.        ]\n",
      "state value of state s4 is 1.3631054687500002\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-1.801125   -1.55863125 -1.06782187  1.22679492  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-2.00125    -1.7318125  -1.18646875  1.36310547  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-2.801125   -2.55863125 -2.06782187  0.22679492 -1.        ]\n",
      "state value of state s1 is -2.55717578125\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-3.3014582  -2.55863125 -2.06782187  0.22679492 -1.        ]\n",
      "state value of state s2 is -2.0479814453125\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-3.3014582  -2.8431833  -2.06782187  0.22679492 -1.        ]\n",
      "state value of state s3 is -1.2289225585937498\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -3.3014582   -2.8431833   -2.1060303    0.22679492  10.        ]\n",
      "state value of state s4 is 1.3193953295898437\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-2.3014582 -1.8431833 -1.1060303  1.1874558  0.       ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-2.55717578 -2.04798145 -1.22892256  1.31939533  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-3.3014582 -2.8431833 -2.1060303  0.1874558 -1.       ]\n",
      "state value of state s1 is -2.888032502441406\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-3.59922925 -2.8431833  -2.1060303   0.1874558  -1.        ]\n",
      "state value of state s2 is -2.2745350142822267\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-3.59922925 -3.04708151 -2.1060303   0.1874558  -1.        ]\n",
      "state value of state s3 is -1.3325869904174803\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -3.59922925  -3.04708151  -2.19932829   0.1874558   10.        ]\n",
      "state value of state s4 is 1.2352614981002805\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-2.59922925 -2.04708151 -1.19932829  1.11173535  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-2.8880325  -2.27453501 -1.33258699  1.2352615   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-3.59922925 -3.04708151 -2.19932829  0.11173535 -1.        ]\n",
      "state value of state s1 is -3.1112170771560668\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-3.80009537 -3.04708151 -2.19932829  0.11173535 -1.        ]\n",
      "state value of state s2 is -2.445630761714554\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-3.80009537 -3.20106769 -2.19932829  0.11173535 -1.        ]\n",
      "state value of state s3 is -1.443988241058922\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -3.80009537  -3.20106769  -2.29958942   0.11173535  10.        ]\n",
      "state value of state s4 is 1.152769561448531\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-2.80009537 -2.20106769 -1.29958942  1.03749261  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.11121708 -2.44563076 -1.44398824  1.15276956  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-3.80009537 -3.20106769 -2.29958942  0.03749261 -1.        ]\n",
      "state value of state s1 is -3.2752119603442624\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-3.94769076 -3.20106769 -2.29958942  0.03749261 -1.        ]\n",
      "state value of state s2 is -2.578083382523089\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-3.94769076 -3.32027504 -2.29958942  0.03749261 -1.        ]\n",
      "state value of state s3 is -1.5430737426638776\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -3.94769076  -3.32027504  -2.38876637   0.03749261  10.        ]\n",
      "state value of state s4 is 1.0821127981588519\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-2.94769076 -2.32027504 -1.38876637  0.97390152  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.27521196 -2.57808338 -1.54307374  1.0821128   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-3.94769076 -3.32027504 -2.38876637 -0.02609848 -1.        ]\n",
      "state value of state s1 is -3.401105735321986\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.06099516 -3.32027504 -2.38876637 -0.02609848 -1.        ]\n",
      "state value of state s2 is -2.6819109329970954\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.06099516 -3.41371984 -2.38876637 -0.02609848 -1.        ]\n",
      "state value of state s3 is -1.6254896233753358\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.06099516  -3.41371984  -2.46294066  -0.02609848  10.        ]\n",
      "state value of state s4 is 1.0243102544019447\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.06099516 -2.41371984 -1.46294066  0.92187923  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.40110574 -2.68191093 -1.62548962  1.02431025  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.06099516 -3.41371984 -2.46294066 -0.07812077 -1.        ]\n",
      "state value of state s1 is -3.4996627060786905\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.14969644 -3.41371984 -2.46294066 -0.07812077 -1.        ]\n",
      "state value of state s2 is -2.763814221475961\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.14969644 -3.4874328  -2.46294066 -0.07812077 -1.        ]\n",
      "state value of state s3 is -1.6922196596462808\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.14969644  -3.4874328   -2.52299769  -0.07812077  10.        ]\n",
      "state value of state s4 is 0.9778621839879331\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.14969644 -2.4874328  -1.52299769  0.88007597  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.49966271 -2.76381422 -1.69221966  0.97786218  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.14969644 -3.4874328  -2.52299769 -0.11992403 -1.        ]\n",
      "state value of state s1 is -3.5774558409879154\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.21971026 -3.4874328  -2.52299769 -0.11992403 -1.        ]\n",
      "state value of state s2 is -2.8286249724891785\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.21971026 -3.54576248 -2.52299769 -0.11992403 -1.        ]\n",
      "state value of state s3 is -1.7456390048481243\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.21971026  -3.54576248  -2.5710751   -0.11992403  10.        ]\n",
      "state value of state s4 is 0.9408095964963918\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.21971026 -2.54576248 -1.5710751   0.84672864  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.57745584 -2.82862497 -1.745639    0.9408096   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.21971026 -3.54576248 -2.5710751  -0.15327136 -1.        ]\n",
      "state value of state s1 is -3.6390645233454553\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.27515807 -3.54576248 -2.5710751  -0.15327136 -1.        ]\n",
      "state value of state s2 is -2.87998859616117\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.27515807 -3.59198974 -2.5710751  -0.15327136 -1.        ]\n",
      "state value of state s3 is -1.7881939754201792\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.27515807  -3.59198974  -2.60937458  -0.15327136  10.        ]\n",
      "state value of state s4 is 0.9113410806058846\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.27515807 -2.59198974 -1.60937458  0.82020697  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.63906452 -2.8799886  -1.78819398  0.91134108  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.27515807 -3.59198974 -2.60937458 -0.17979303 -1.        ]\n",
      "state value of state s1 is -3.6879201141112583\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.3191281  -3.59198974 -2.60937458 -0.17979303 -1.        ]\n",
      "state value of state s2 is -2.9207251508112355\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.3191281  -3.62865264 -2.60937458 -0.17979303 -1.        ]\n",
      "state value of state s3 is -1.8220221838719253\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.3191281   -3.62865264  -2.63981997  -0.17979303  10.        ]\n",
      "state value of state s4 is 0.8879335928326131\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.3191281  -2.62865264 -1.63981997  0.79914023  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.68792011 -2.92072515 -1.82202218  0.88793359  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.3191281  -3.62865264 -2.63981997 -0.20085977 -1.        ]\n",
      "state value of state s1 is -3.7266822016537775\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.35401398 -3.62865264 -2.63981997 -0.20085977 -1.        ]\n",
      "state value of state s2 is -2.953044754849818\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.35401398 -3.65774028 -2.63981997 -0.20085977 -1.        ]\n",
      "state value of state s3 is -1.8488883699686074\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.35401398  -3.65774028  -2.66399953  -0.20085977  10.        ]\n",
      "state value of state s4 is 0.8693501053031922\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.35401398 -2.65774028 -1.66399953  0.78241509  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.7266822  -2.95304475 -1.84888837  0.86935011  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.35401398 -3.65774028 -2.66399953 -0.21758491 -1.        ]\n",
      "state value of state s1 is -3.7574419438283457\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.38169775 -3.65774028 -2.66399953 -0.21758491 -1.        ]\n",
      "state value of state s2 is -2.9786908033505775\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.38169775 -3.68082172 -2.66399953 -0.21758491 -1.        ]\n",
      "state value of state s3 is -1.870216773217878\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.38169775  -3.68082172  -2.6831951   -0.21758491  10.        ]\n",
      "state value of state s4 is 0.8545995689653159\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.38169775 -2.68082172 -1.6831951   0.76913961  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.75744194 -2.9786908  -1.87021677  0.85459957  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.38169775 -3.68082172 -2.6831951  -0.23086039 -1.        ]\n",
      "state value of state s1 is -3.781853079450658\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.40366777 -3.68082172 -2.6831951  -0.23086039 -1.        ]\n",
      "state value of state s2 is -2.9990429013669617\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.40366777 -3.69913861 -2.6831951  -0.23086039 -1.        ]\n",
      "state value of state s3 is -1.8871459108160287\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.40366777  -3.69913861  -2.69843132  -0.23086039  10.        ]\n",
      "state value of state s4 is 0.8428924202760233\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.40366777 -2.69913861 -1.69843132  0.75860318  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.78185308 -2.9990429  -1.88714591  0.84289242  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.40366777 -3.69913861 -2.69843132 -0.24139682 -1.        ]\n",
      "state value of state s1 is -3.8012263684939693\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.42110373 -3.69913861 -2.69843132 -0.24139682 -1.        ]\n",
      "state value of state s2 is -3.0151944439641705\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.42110373 -3.713675   -2.69843132 -0.24139682 -1.        ]\n",
      "state value of state s3 is -1.9005821737205393\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.42110373  -3.713675    -2.71052396  -0.24139682  10.        ]\n",
      "state value of state s4 is 0.8336010555830455\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.42110373 -2.713675   -1.71052396  0.75024095  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.80122637 -3.01519444 -1.90058217  0.83360106  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.42110373 -3.713675   -2.71052396 -0.24975905 -1.        ]\n",
      "state value of state s1 is -3.8166016048013462\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.43494144 -3.713675   -2.71052396 -0.24975905 -1.        ]\n",
      "state value of state s2 is -3.0280126233579945\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.43494144 -3.72521136 -2.71052396 -0.24975905 -1.        ]\n",
      "state value of state s3 is -1.9112458751550536\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.43494144  -3.72521136  -2.72012129  -0.24975905  10.        ]\n",
      "state value of state s4 is 0.8262270753407495\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.43494144 -2.72521136 -1.72012129  0.74360437  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.8166016  -3.02801262 -1.91124588  0.82622708  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.43494144 -3.72521136 -2.72012129 -0.25639563 -1.        ]\n",
      "state value of state s1 is -3.8288038843260415\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4459235  -3.72521136 -2.72012129 -0.25639563 -1.        ]\n",
      "state value of state s2 is -3.038185462532788\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4459235  -3.73436692 -2.72012129 -0.25639563 -1.        ]\n",
      "state value of state s3 is -1.9197090119799092\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4459235   -3.73436692  -2.72773811  -0.25639563  10.        ]\n",
      "state value of state s4 is 0.8203748351863116\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4459235  -2.73436692 -1.72773811  0.73833735  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.82880388 -3.03818546 -1.91970901  0.82037484  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4459235  -3.73436692 -2.72773811 -0.26166265 -1.        ]\n",
      "state value of state s1 is -3.838488004712075\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4546392  -3.73436692 -2.72773811 -0.26166265 -1.        ]\n",
      "state value of state s2 is -3.0462589212830515\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4546392  -3.74163303 -2.72773811 -0.26166265 -1.        ]\n",
      "state value of state s3 is -1.9264256529218564\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4546392   -3.74163303  -2.73378309  -0.26166265  10.        ]\n",
      "state value of state s4 is 0.8157303087208159\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4546392  -2.74163303 -1.73378309  0.73415728  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.838488   -3.04625892 -1.92642565  0.81573031  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4546392  -3.74163303 -2.73378309 -0.26584272 -1.        ]\n",
      "state value of state s1 is -3.846173631316538\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.46155627 -3.74163303 -2.73378309 -0.26584272 -1.        ]\n",
      "state value of state s2 is -3.0526662621614107\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.46155627 -3.74739964 -2.73378309 -0.26584272 -1.        ]\n",
      "state value of state s3 is -1.9317562000292714\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.46155627  -3.74739964  -2.73858058  -0.26584272  10.        ]\n",
      "state value of state s4 is 0.8120442654692801\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.46155627 -2.74739964 -1.73858058  0.73083984  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.84617363 -3.05266626 -1.9317562   0.81204427  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.46155627 -3.74739964 -2.73858058 -0.26916016 -1.        ]\n",
      "state value of state s1 is -3.8522731880853454\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.46704587 -3.74739964 -2.73858058 -0.26916016 -1.        ]\n",
      "state value of state s2 is -3.0577513255612496\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.46704587 -3.75197619 -2.73858058 -0.26916016 -1.        ]\n",
      "state value of state s3 is -1.9359866928646128\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.46704587  -3.75197619  -2.74238802  -0.26916016  10.        ]\n",
      "state value of state s4 is 0.809118905584769\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.46704587 -2.75197619 -1.74238802  0.72820702  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.85227319 -3.05775133 -1.93598669  0.80911891  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.46704587 -3.75197619 -2.74238802 -0.27179298 -1.        ]\n",
      "state value of state s1 is -3.8571139887842243\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.47140259 -3.75197619 -2.74238802 -0.27179298 -1.        ]\n",
      "state value of state s2 is -3.0617869902224397\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.47140259 -3.75560829 -2.74238802 -0.27179298 -1.        ]\n",
      "state value of state s3 is -1.9393441458578422\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.47140259  -3.75560829  -2.74540973  -0.27179298  10.        ]\n",
      "state value of state s4 is 0.8067972481385097\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.47140259 -2.75560829 -1.74540973  0.72611752  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.85711399 -3.06178699 -1.93934415  0.80679725  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.47140259 -3.75560829 -2.74540973 -0.27388248 -1.        ]\n",
      "state value of state s1 is -3.8609558005709648\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.47486022 -3.75560829 -2.74540973 -0.27388248 -1.        ]\n",
      "state value of state s2 is -3.0649898198974004\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.47486022 -3.75849084 -2.74540973 -0.27388248 -1.        ]\n",
      "state value of state s3 is -1.9420087262841523\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.47486022  -3.75849084  -2.74780785  -0.27388248  10.        ]\n",
      "state value of state s4 is 0.8049547079403152\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.47486022 -2.75849084 -1.74780785  0.72445924  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.8609558  -3.06498982 -1.94200873  0.80495471  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.47486022 -3.75849084 -2.74780785 -0.27554076 -1.        ]\n",
      "state value of state s1 is -3.8640047831477835\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4776043  -3.75849084 -2.74780785 -0.27554076 -1.        ]\n",
      "state value of state s2 is -3.0675316858755104\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4776043  -3.76077852 -2.74780785 -0.27554076 -1.        ]\n",
      "state value of state s3 is -1.944123421049044\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4776043   -3.76077852  -2.74971108  -0.27554076  10.        ]\n",
      "state value of state s4 is 0.8034924102285461\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4776043  -2.76077852 -1.74971108  0.72314317  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.86400478 -3.06753169 -1.94412342  0.80349241  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4776043  -3.76077852 -2.74971108 -0.27685683 -1.        ]\n",
      "state value of state s1 is -3.8664245514745272\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4797821  -3.76077852 -2.74971108 -0.27685683 -1.        ]\n",
      "state value of state s2 is -3.0695489904243254\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4797821  -3.76259409 -2.74971108 -0.27685683 -1.        ]\n",
      "state value of state s3 is -1.9458017092149578\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4797821   -3.76259409  -2.75122154  -0.27685683  10.        ]\n",
      "state value of state s4 is 0.8023318848825842\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4797821  -2.76259409 -1.75122154  0.7220987   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.86642455 -3.06954899 -1.94580171  0.80233188  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4797821  -3.76259409 -2.75122154 -0.2779013  -1.        ]\n",
      "state value of state s1 is -3.868344955582376\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48151046 -3.76259409 -2.75122154 -0.2779013  -1.        ]\n",
      "state value of state s2 is -3.0711499865983996\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48151046 -3.76403499 -2.75122154 -0.2779013  -1.        ]\n",
      "state value of state s3 is -1.9471336513822373\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48151046  -3.76403499  -2.75242029  -0.2779013   10.        ]\n",
      "state value of state s4 is 0.8014108555529385\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48151046 -2.76403499 -1.75242029  0.72126977  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.86834496 -3.07114999 -1.94713365  0.80141086  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48151046 -3.76403499 -2.75242029 -0.27873023 -1.        ]\n",
      "state value of state s1 is -3.869869048557712\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48288214 -3.76403499 -2.75242029 -0.27873023 -1.        ]\n",
      "state value of state s2 is -3.072420587395354\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48288214 -3.76517853 -2.75242029 -0.27873023 -1.        ]\n",
      "state value of state s3 is -1.9481907224876662\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48288214  -3.76517853  -2.75337165  -0.27873023  10.        ]\n",
      "state value of state s4 is 0.8006798977757317\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48288214 -2.76517853 -1.75337165  0.72061191  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.86986905 -3.07242059 -1.94819072  0.8006799   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48288214 -3.76517853 -2.75337165 -0.27938809 -1.        ]\n",
      "state value of state s1 is -3.87107861657465\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48397075 -3.76517853 -2.75337165 -0.27938809 -1.        ]\n",
      "state value of state s2 is -3.0734289760576656\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48397075 -3.76608608 -2.75337165 -0.27938809 -1.        ]\n",
      "state value of state s3 is -1.949029647289942\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48397075  -3.76608608  -2.75412668  -0.27938809  10.        ]\n",
      "state value of state s4 is 0.800099786746328\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48397075 -2.76608608 -1.75412668  0.72008981  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87107862 -3.07342898 -1.94902965  0.80009979  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48397075 -3.76608608 -2.75412668 -0.27991019 -1.        ]\n",
      "state value of state s1 is -3.872038567711804\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48483471 -3.76608608 -2.75412668 -0.27991019 -1.        ]\n",
      "state value of state s2 is -3.074229264943182\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48483471 -3.76680634 -2.75412668 -0.27991019 -1.        ]\n",
      "state value of state s3 is -1.9496954443395453\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48483471  -3.76680634  -2.7547259   -0.27991019  10.        ]\n",
      "state value of state s4 is 0.7996393924293101\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48483471 -2.76680634 -1.7547259   0.71967545  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87203857 -3.07422926 -1.94969544  0.79963939  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48483471 -3.76680634 -2.7547259  -0.28032455 -1.        ]\n",
      "state value of state s1 is -3.8728004150589257\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48552037 -3.76680634 -2.7547259  -0.28032455 -1.        ]\n",
      "state value of state s2 is -3.0748643993160956\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48552037 -3.76737796 -2.7547259  -0.28032455 -1.        ]\n",
      "state value of state s3 is -1.9502238417714663\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48552037  -3.76737796  -2.75520146  -0.28032455  10.        ]\n",
      "state value of state s4 is 0.7992740090518935\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48552037 -2.76737796 -1.75520146  0.71934661  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87280042 -3.0748644  -1.95022384  0.79927401  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48552037 -3.76737796 -2.75520146 -0.28065339 -1.        ]\n",
      "state value of state s1 is -3.873405041021218\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48606454 -3.76737796 -2.75520146 -0.28065339 -1.        ]\n",
      "state value of state s2 is -3.075368461885341\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48606454 -3.76783162 -2.75520146 -0.28065339 -1.        ]\n",
      "state value of state s3 is -1.9506431945550018\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48606454  -3.76783162  -2.75557888  -0.28065339  10.        ]\n",
      "state value of state s4 is 0.7989840293375989\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48606454 -2.76783162 -1.75557888  0.71908563  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87340504 -3.07536846 -1.95064319  0.79898403  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48606454 -3.76783162 -2.75557888 -0.28091437 -1.        ]\n",
      "state value of state s1 is -3.8738848911586254\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4864964  -3.76783162 -2.75557888 -0.28091437 -1.        ]\n",
      "state value of state s2 is -3.0757685017581347\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4864964  -3.76819165 -2.75557888 -0.28091437 -1.        ]\n",
      "state value of state s3 is -1.9509760060836465\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4864964   -3.76819165  -2.75587841  -0.28091437  10.        ]\n",
      "state value of state s4 is 0.7987538923365589\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4864964  -2.76819165 -1.75587841  0.7188785   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87388489 -3.0757685  -1.95097601  0.79875389  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4864964  -3.76819165 -2.75587841 -0.2811215  -1.        ]\n",
      "state value of state s1 is -3.874265715285782\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48683914 -3.76819165 -2.75587841 -0.2811215  -1.        ]\n",
      "state value of state s2 is -3.0760859859547356\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48683914 -3.76847739 -2.75587841 -0.2811215  -1.        ]\n",
      "state value of state s3 is -1.9512401357566698\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48683914  -3.76847739  -2.75611612  -0.2811215   10.        ]\n",
      "state value of state s4 is 0.7985712483906595\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48683914 -2.76847739 -1.75611612  0.71871412  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87426572 -3.07608599 -1.95124014  0.79857125  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48683914 -3.76847739 -2.75611612 -0.28128588 -1.        ]\n",
      "state value of state s1 is -3.8745679492636675\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48711115 -3.76847739 -2.75611612 -0.28128588 -1.        ]\n",
      "state value of state s2 is -3.076337951376058\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48711115 -3.76870416 -2.75611612 -0.28128588 -1.        ]\n",
      "state value of state s3 is -1.951449757353779\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48711115  -3.76870416  -2.75630478  -0.28128588  10.        ]\n",
      "state value of state s4 is 0.798426296423685\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48711115 -2.76870416 -1.75630478  0.71858367  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87456795 -3.07633795 -1.95144976  0.7984263   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48711115 -3.76870416 -2.75630478 -0.28141633 -1.        ]\n",
      "state value of state s1 is -3.8748078116328637\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48732703 -3.76870416 -2.75630478 -0.28141633 -1.        ]\n",
      "state value of state s2 is -3.076537919041291\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48732703 -3.76888413 -2.75630478 -0.28141633 -1.        ]\n",
      "state value of state s3 is -1.9516161196313364\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48732703  -3.76888413  -2.75645451  -0.28141633  10.        ]\n",
      "state value of state s4 is 0.7983112579939879\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48732703 -2.76888413 -1.75645451  0.71848013  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87480781 -3.07653792 -1.95161612  0.79831126  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48732703 -3.76888413 -2.75645451 -0.28151987 -1.        ]\n",
      "state value of state s1 is -3.87499817393613\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48749836 -3.76888413 -2.75645451 -0.28151987 -1.        ]\n",
      "state value of state s2 is -3.076696619655563\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48749836 -3.76902696 -2.75645451 -0.28151987 -1.        ]\n",
      "state value of state s3 is -1.9517481499553853\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48749836  -3.76902696  -2.75657333  -0.28151987  10.        ]\n",
      "state value of state s4 is 0.7982199598861839\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48749836 -2.76902696 -1.75657333  0.71839796  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87499817 -3.07669662 -1.95174815  0.79821996  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48749836 -3.76902696 -2.75657333 -0.28160204 -1.        ]\n",
      "state value of state s1 is -3.8751492514337214\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48763433 -3.76902696 -2.75657333 -0.28160204 -1.        ]\n",
      "state value of state s2 is -3.076822569443199\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48763433 -3.76914031 -2.75657333 -0.28160204 -1.        ]\n",
      "state value of state s3 is -1.9518529333637664\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48763433  -3.76914031  -2.75666764  -0.28160204  10.        ]\n",
      "state value of state s4 is 0.7981475028428242\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bellman equations [-3.48763433 -2.76914031 -1.75666764  0.71833275  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87514925 -3.07682257 -1.95185293  0.7981475   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48763433 -3.76914031 -2.75666764 -0.28166725 -1.        ]\n",
      "state value of state s1 is -3.875269151276742\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48774224 -3.76914031 -2.75666764 -0.28166725 -1.        ]\n",
      "state value of state s2 is -3.076922527147071\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48774224 -3.76923027 -2.75666764 -0.28166725 -1.        ]\n",
      "state value of state s3 is -1.9519360927648433\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48774224  -3.76923027  -2.75674248  -0.28166725  10.        ]\n",
      "state value of state s4 is 0.7980899986594547\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48774224 -2.76923027 -1.75674248  0.718281    0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87526915 -3.07692253 -1.95193609  0.79809     0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48774224 -3.76923027 -2.75674248 -0.281719   -1.        ]\n",
      "state value of state s1 is -3.8753643075547144\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48782788 -3.76923027 -2.75674248 -0.281719   -1.        ]\n",
      "state value of state s2 is -3.077001856717615\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48782788 -3.76930167 -2.75674248 -0.281719   -1.        ]\n",
      "state value of state s3 is -1.9520020906751459\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48782788  -3.76930167  -2.75680188  -0.281719    10.        ]\n",
      "state value of state s4 is 0.7980443615350061\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48782788 -2.76930167 -1.75680188  0.71823993  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87536431 -3.07700186 -1.95200209  0.79804436  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48782788 -3.76930167 -2.75680188 -0.28176007 -1.        ]\n",
      "state value of state s1 is -3.8754398265629932\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48789584 -3.76930167 -2.75680188 -0.28176007 -1.        ]\n",
      "state value of state s2 is -3.0770648151542237\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48789584 -3.76935833 -2.75680188 -0.28176007 -1.        ]\n",
      "state value of state s3 is -1.9520544686878285\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48789584  -3.76935833  -2.75684902  -0.28176007  10.        ]\n",
      "state value of state s4 is 0.7980081424809145\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48789584 -2.76935833 -1.75684902  0.71820733  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87543983 -3.07706482 -1.95205447  0.79800814  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48789584 -3.76935833 -2.75684902 -0.28179267 -1.        ]\n",
      "state value of state s1 is -3.8754997608178092\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48794978 -3.76935833 -2.75684902 -0.28179267 -1.        ]\n",
      "state value of state s2 is -3.077114780945202\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48794978 -3.7694033  -2.75684902 -0.28179267 -1.        ]\n",
      "state value of state s3 is -1.952096037522357\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48794978  -3.7694033   -2.75688643  -0.28179267  10.        ]\n",
      "state value of state s4 is 0.7979793979030048\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48794978 -2.7694033  -1.75688643  0.71818146  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87549976 -3.07711478 -1.95209604  0.7979794   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48794978 -3.7694033  -2.75688643 -0.28181854 -1.        ]\n",
      "state value of state s1 is -3.875547326523215\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48799259 -3.7694033  -2.75688643 -0.28181854 -1.        ]\n",
      "state value of state s2 is -3.0771544353648883\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48799259 -3.76943899 -2.75688643 -0.28181854 -1.        ]\n",
      "state value of state s3 is -1.9521290278539019\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48799259  -3.76943899  -2.75691613  -0.28181854  10.        ]\n",
      "state value of state s4 is 0.7979565853039485\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48799259 -2.76943899 -1.75691613  0.71816093  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87554733 -3.07715444 -1.95212903  0.79795659  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48799259 -3.76943899 -2.75691613 -0.28183907 -1.        ]\n",
      "state value of state s1 is -3.875585076159675\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48802657 -3.76943899 -2.75691613 -0.28183907 -1.        ]\n",
      "state value of state s2 is -3.077185906356738\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48802657 -3.76946732 -2.75691613 -0.28183907 -1.        ]\n",
      "state value of state s3 is -1.952155210016278\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48802657  -3.76946732  -2.75693969  -0.28183907  10.        ]\n",
      "state value of state s4 is 0.7979384805094598\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48802657 -2.76946732 -1.75693969  0.71814463  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87558508 -3.07718591 -1.95215521  0.79793848  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48802657 -3.76946732 -2.75693969 -0.28185537 -1.        ]\n",
      "state value of state s1 is -3.8756150354557826\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48805353 -3.76946732 -2.75693969 -0.28185537 -1.        ]\n",
      "state value of state s2 is -3.0772108827234543\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48805353 -3.76948979 -2.75693969 -0.28185537 -1.        ]\n",
      "state value of state s3 is -1.9521759890019568\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48805353  -3.76948979  -2.75695839  -0.28185537  10.        ]\n",
      "state value of state s4 is 0.7979241119764109\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48805353 -2.76948979 -1.75695839  0.7181317   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87561504 -3.07721088 -1.95217599  0.79792411  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48805353 -3.76948979 -2.75695839 -0.2818683  -1.        ]\n",
      "state value of state s1 is -3.8756388120933196\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48807493 -3.76948979 -2.75695839 -0.2818683  -1.        ]\n",
      "state value of state s2 is -3.077230704751859\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48807493 -3.76950763 -2.75695839 -0.2818683  -1.        ]\n",
      "state value of state s3 is -1.9521924798570525\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48807493  -3.76950763  -2.75697323  -0.2818683   10.        ]\n",
      "state value of state s4 is 0.7979127086576874\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48807493 -2.76950763 -1.75697323  0.71812144  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87563881 -3.0772307  -1.95219248  0.79791271  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48807493 -3.76950763 -2.75697323 -0.28187856 -1.        ]\n",
      "state value of state s1 is -3.875657681978999\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48809191 -3.76950763 -2.75697323 -0.28187856 -1.        ]\n",
      "state value of state s2 is -3.0772464361356318\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48809191 -3.76952179 -2.75697323 -0.28187856 -1.        ]\n",
      "state value of state s3 is -1.9522055675171524\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48809191  -3.76952179  -2.75698501  -0.28187856  10.        ]\n",
      "state value of state s4 is 0.7979036586261032\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48809191 -2.76952179 -1.75698501  0.71811329  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87565768 -3.07724644 -1.95220557  0.79790366  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48809191 -3.76952179 -2.75698501 -0.28188671 -1.        ]\n",
      "state value of state s1 is -3.875672657712426\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48810539 -3.76952179 -2.75698501 -0.28188671 -1.        ]\n",
      "state value of state s2 is -3.077258921055457\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48810539 -3.76953303 -2.75698501 -0.28188671 -1.        ]\n",
      "state value of state s3 is -1.952215954294909\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48810539  -3.76953303  -2.75699436  -0.28188671  10.        ]\n",
      "state value of state s4 is 0.7978964762370409\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48810539 -2.76953303 -1.75699436  0.71810683  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87567266 -3.07725892 -1.95221595  0.79789648  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48810539 -3.76953303 -2.75699436 -0.28189317 -1.        ]\n",
      "state value of state s1 is -3.875684542924424\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48811609 -3.76953303 -2.75699436 -0.28189317 -1.        ]\n",
      "state value of state s2 is -3.0772688294796167\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48811609 -3.76954195 -2.75699436 -0.28189317 -1.        ]\n",
      "state value of state s3 is -1.9522241975676815\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48811609  -3.76954195  -2.75700178  -0.28189317  10.        ]\n",
      "state value of state s4 is 0.7978907760676921\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48811609 -2.76954195 -1.75700178  0.7181017   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87568454 -3.07726883 -1.9522242   0.79789078  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48811609 -3.76954195 -2.75700178 -0.2818983  -1.        ]\n",
      "state value of state s1 is -3.875693975401633\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48812458 -3.76954195 -2.75700178 -0.2818983  -1.        ]\n",
      "state value of state s2 is -3.077276693115964\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48812458 -3.76954902 -2.75700178 -0.2818983  -1.        ]\n",
      "state value of state s3 is -1.9522307396876344\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48812458  -3.76954902  -2.75700767  -0.2818983   10.        ]\n",
      "state value of state s4 is 0.797886252234421\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48812458 -2.76954902 -1.75700767  0.71809763  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87569398 -3.07727669 -1.95223074  0.79788625  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48812458 -3.76954902 -2.75700767 -0.28190237 -1.        ]\n",
      "state value of state s1 is -3.875701461311545\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48813132 -3.76954902 -2.75700767 -0.28190237 -1.        ]\n",
      "state value of state s2 is -3.0772829339445367\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48813132 -3.76955464 -2.75700767 -0.28190237 -1.        ]\n",
      "state value of state s3 is -1.9522359317193259\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48813132  -3.76955464  -2.75701234  -0.28190237  10.        ]\n",
      "state value of state s4 is 0.7978826619783757\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48813132 -2.76955464 -1.75701234  0.7180944   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87570146 -3.07728293 -1.95223593  0.79788266  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48813132 -3.76955464 -2.75701234 -0.2819056  -1.        ]\n",
      "state value of state s1 is -3.875707402364564\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48813666 -3.76955464 -2.75701234 -0.2819056  -1.        ]\n",
      "state value of state s2 is -3.0772878868619338\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48813666 -3.7695591  -2.75701234 -0.2819056  -1.        ]\n",
      "state value of state s3 is -1.952240052278606\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48813666  -3.7695591   -2.75701605  -0.2819056   10.        ]\n",
      "state value of state s4 is 0.7978798126385129\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48813666 -2.7695591  -1.75701605  0.71809183  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.8757074  -3.07728789 -1.95224005  0.79787981  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48813666 -3.7695591  -2.75701605 -0.28190817 -1.        ]\n",
      "state value of state s1 is -3.8757121173706754\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48814091 -3.7695591  -2.75701605 -0.28190817 -1.        ]\n",
      "state value of state s2 is -3.077291817652607\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48814091 -3.76956264 -2.75701605 -0.28190817 -1.        ]\n",
      "state value of state s3 is -1.9522433224837576\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48814091  -3.76956264  -2.75701899  -0.28190817  10.        ]\n",
      "state value of state s4 is 0.7978775513129832\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48814091 -2.76956264 -1.75701899  0.7180898   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87571212 -3.07729182 -1.95224332  0.79787755  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48814091 -3.76956264 -2.75701899 -0.2819102  -1.        ]\n",
      "state value of state s1 is -3.8757158593474865\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48814427 -3.76956264 -2.75701899 -0.2819102  -1.        ]\n",
      "state value of state s2 is -3.077294937251436\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48814427 -3.76956544 -2.75701899 -0.2819102  -1.        ]\n",
      "state value of state s3 is -1.9522459178211875\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48814427  -3.76956544  -2.75702133  -0.2819102   10.        ]\n",
      "state value of state s4 is 0.7978757566540811\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48814427 -2.76956544 -1.75702133  0.71808818  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87571586 -3.07729494 -1.95224592  0.79787576  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48814427 -3.76956544 -2.75702133 -0.28191182 -1.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state value of state s1 is -3.8757188290977096\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48814695 -3.76956544 -2.75702133 -0.28191182 -1.        ]\n",
      "state value of state s2 is -3.077297413062963\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48814695 -3.76956767 -2.75702133 -0.28191182 -1.        ]\n",
      "state value of state s3 is -1.9522479775624153\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48814695  -3.76956767  -2.75702318  -0.28191182  10.        ]\n",
      "state value of state s4 is 0.7978743323564581\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48814695 -2.76956767 -1.75702318  0.7180869   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87571883 -3.07729741 -1.95224798  0.79787433  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48814695 -3.76956767 -2.75702318 -0.2819131  -1.        ]\n",
      "state value of state s1 is -3.8757211859846796\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48814907 -3.76956767 -2.75702318 -0.2819131  -1.        ]\n",
      "state value of state s2 is -3.0772993779446836\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48814907 -3.76956944 -2.75702318 -0.2819131  -1.        ]\n",
      "state value of state s3 is -1.9522496122376902\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48814907  -3.76956944  -2.75702465  -0.2819131   10.        ]\n",
      "state value of state s4 is 0.7978732019891688\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48814907 -2.76956944 -1.75702465  0.71808588  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572119 -3.07729938 -1.95224961  0.7978732   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48814907 -3.76956944 -2.75702465 -0.28191412 -1.        ]\n",
      "state value of state s1 is -3.8757230564841403\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815075 -3.76956944 -2.75702465 -0.28191412 -1.        ]\n",
      "state value of state s2 is -3.077300937336476\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815075 -3.76957084 -2.75702465 -0.28191412 -1.        ]\n",
      "state value of state s3 is -1.952250909567286\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815075  -3.76957084  -2.75702582  -0.28191412  10.        ]\n",
      "state value of state s4 is 0.7978723048942165\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815075 -2.76957084 -1.75702582  0.71808507  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572306 -3.07730094 -1.95225091  0.7978723   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815075 -3.76957084 -2.75702582 -0.28191493 -1.        ]\n",
      "state value of state s1 is -3.8757245409712096\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815209 -3.76957084 -2.75702582 -0.28191493 -1.        ]\n",
      "state value of state s2 is -3.0773021749187377\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815209 -3.76957196 -2.75702582 -0.28191493 -1.        ]\n",
      "state value of state s3 is -1.952251939168764\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815209  -3.76957196  -2.75702675  -0.28191493  10.        ]\n",
      "state value of state s4 is 0.7978715929315108\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815209 -2.76957196 -1.75702675  0.71808443  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572454 -3.07730217 -1.95225194  0.79787159  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815209 -3.76957196 -2.75702675 -0.28191557 -1.        ]\n",
      "state value of state s1 is -3.8757257191067325\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815315 -3.76957196 -2.75702675 -0.28191557 -1.        ]\n",
      "state value of state s2 is -3.0773031571028566\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815315 -3.76957284 -2.75702675 -0.28191557 -1.        ]\n",
      "state value of state s3 is -1.952252756292807\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815315  -3.76957284  -2.75702748  -0.28191557  10.        ]\n",
      "state value of state s4 is 0.7978710278955656\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815315 -2.76957284 -1.75702748  0.71808393  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572572 -3.07730316 -1.95225276  0.79787103  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815315 -3.76957284 -2.75702748 -0.28191607 -1.        ]\n",
      "state value of state s1 is -3.875726654112054\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815399 -3.76957284 -2.75702748 -0.28191607 -1.        ]\n",
      "state value of state s2 is -3.0773039365949955\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815399 -3.76957354 -2.75702748 -0.28191607 -1.        ]\n",
      "state value of state s3 is -1.9522534047880893\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815399  -3.76957354  -2.75702806  -0.28191607  10.        ]\n",
      "state value of state s4 is 0.797870579465308\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815399 -2.76957354 -1.75702806  0.71808352  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572665 -3.07730394 -1.9522534   0.79787058  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815399 -3.76957354 -2.75702806 -0.28191648 -1.        ]\n",
      "state value of state s1 is -3.8757273961616185\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815466 -3.76957354 -2.75702806 -0.28191648 -1.        ]\n",
      "state value of state s2 is -3.077304555224418\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815466 -3.7695741  -2.75702806 -0.28191648 -1.        ]\n",
      "state value of state s3 is -1.9522539194542956\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815466  -3.7695741   -2.75702853  -0.28191648  10.        ]\n",
      "state value of state s4 is 0.7978702235769839\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815466 -2.7695741  -1.75702853  0.7180832   0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.8757274  -3.07730456 -1.95225392  0.79787022  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815466 -3.7695741  -2.75702853 -0.2819168  -1.        ]\n",
      "state value of state s1 is -3.8757279850754385\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815519 -3.7695741  -2.75702853 -0.2819168  -1.        ]\n",
      "state value of state s2 is -3.0773050461881404\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815519 -3.76957454 -2.75702853 -0.2819168  -1.        ]\n",
      "state value of state s3 is -1.9522543279095474\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815519  -3.76957454  -2.7570289   -0.2819168   10.        ]\n",
      "state value of state s4 is 0.7978699411328416\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815519 -2.76957454 -1.7570289   0.71808295  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572799 -3.07730505 -1.95225433  0.79786994  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815519 -3.76957454 -2.7570289  -0.28191705 -1.        ]\n",
      "state value of state s1 is -3.875728452455927\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815561 -3.76957454 -2.7570289  -0.28191705 -1.        ]\n",
      "state value of state s2 is -3.0773054358323577\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815561 -3.76957489 -2.7570289  -0.28191705 -1.        ]\n",
      "state value of state s3 is -1.952254652072453\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815561  -3.76957489  -2.75702919  -0.28191705  10.        ]\n",
      "state value of state s4 is 0.7978697169763069\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815561 -2.76957489 -1.75702919  0.71808275  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572845 -3.07730544 -1.95225465  0.79786972  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815561 -3.76957489 -2.75702919 -0.28191725 -1.        ]\n",
      "state value of state s1 is -3.8757288233837492\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815594 -3.76957489 -2.75702919 -0.28191725 -1.        ]\n",
      "state value of state s2 is -3.0773057450662353\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815594 -3.76957517 -2.75702919 -0.28191725 -1.        ]\n",
      "state value of state s3 is -1.9522549093383073\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815594  -3.76957517  -2.75702942  -0.28191725  10.        ]\n",
      "state value of state s4 is 0.7978695390786468\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815594 -2.76957517 -1.75702942  0.71808259  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572882 -3.07730575 -1.95225491  0.79786954  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815594 -3.76957517 -2.75702942 -0.28191741 -1.        ]\n",
      "state value of state s1 is -3.875729117763709\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815621 -3.76957517 -2.75702942 -0.28191741 -1.        ]\n",
      "state value of state s2 is -3.077305990483945\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815621 -3.76957539 -2.75702942 -0.28191741 -1.        ]\n",
      "state value of state s3 is -1.9522551135125625\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815621  -3.76957539  -2.7570296   -0.28191741  10.        ]\n",
      "state value of state s4 is 0.7978693978934812\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815621 -2.76957539 -1.7570296   0.71808246  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572912 -3.07730599 -1.95225511  0.7978694   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815621 -3.76957539 -2.7570296  -0.28191754 -1.        ]\n",
      "state value of state s1 is -3.8757293513928834\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815642 -3.76957539 -2.7570296  -0.28191754 -1.        ]\n",
      "state value of state s2 is -3.0773061852551407\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815642 -3.76957557 -2.7570296  -0.28191754 -1.        ]\n",
      "state value of state s3 is -1.952255275551659\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815642  -3.76957557  -2.75702975  -0.28191754  10.        ]\n",
      "state value of state s4 is 0.7978692858445033\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815642 -2.76957557 -1.75702975  0.71808236  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572935 -3.07730619 -1.95225528  0.79786929  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815642 -3.76957557 -2.75702975 -0.28191764 -1.        ]\n",
      "state value of state s1 is -3.8757295368083273\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815658 -3.76957557 -2.75702975 -0.28191764 -1.        ]\n",
      "state value of state s2 is -3.0773063398316736\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815658 -3.76957571 -2.75702975 -0.28191764 -1.        ]\n",
      "state value of state s3 is -1.9522554041509705\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815658  -3.76957571  -2.75702986  -0.28191764  10.        ]\n",
      "state value of state s4 is 0.7978691969189184\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815658 -2.76957571 -1.75702986  0.71808228  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572954 -3.07730634 -1.9522554   0.7978692   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815658 -3.76957571 -2.75702986 -0.28191772 -1.        ]\n",
      "state value of state s1 is -3.8757296839598423\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815672 -3.76957571 -2.75702986 -0.28191772 -1.        ]\n",
      "state value of state s2 is -3.077306462508461\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815672 -3.76957582 -2.75702986 -0.28191772 -1.        ]\n",
      "state value of state s3 is -1.9522555062114195\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815672  -3.76957582  -2.75702996  -0.28191772  10.        ]\n",
      "state value of state s4 is 0.7978691263447837\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815672 -2.76957582 -1.75702996  0.71808221  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572968 -3.07730646 -1.95225551  0.79786913  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815672 -3.76957582 -2.75702996 -0.28191779 -1.        ]\n",
      "state value of state s1 is -3.875729800743902\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815682 -3.76957582 -2.75702996 -0.28191779 -1.        ]\n",
      "state value of state s2 is -3.077306559868609\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815682 -3.7695759  -2.75702996 -0.28191779 -1.        ]\n",
      "state value of state s3 is -1.9522555872097946\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815682  -3.7695759   -2.75703003  -0.28191779  10.        ]\n",
      "state value of state s4 is 0.7978690703349356\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815682 -2.7695759  -1.75703003  0.71808216  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.8757298  -3.07730656 -1.95225559  0.79786907  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815682 -3.7695759  -2.75703003 -0.28191784 -1.        ]\n",
      "state value of state s1 is -3.8757298934273967\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4881569  -3.7695759  -2.75703003 -0.28191784 -1.        ]\n",
      "state value of state s2 is -3.0773066371366777\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4881569  -3.76957597 -2.75703003 -0.28191784 -1.        ]\n",
      "state value of state s3 is -1.952255651492647\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4881569   -3.76957597  -2.75703009  -0.28191784  10.        ]\n",
      "state value of state s4 is 0.7978690258837626\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4881569  -2.76957597 -1.75703009  0.71808212  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572989 -3.07730664 -1.95225565  0.79786903  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4881569  -3.76957597 -2.75703009 -0.28191788 -1.        ]\n",
      "state value of state s1 is -3.8757299669839265\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815697 -3.76957597 -2.75703009 -0.28191788 -1.        ]\n",
      "state value of state s2 is -3.0773066984590414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815697 -3.76957603 -2.75703009 -0.28191788 -1.        ]\n",
      "state value of state s3 is -1.952255702509536\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815697  -3.76957603  -2.75703013  -0.28191788  10.        ]\n",
      "state value of state s4 is 0.7978689906059167\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815697 -2.76957603 -1.75703013  0.71808209  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87572997 -3.0773067  -1.9522557   0.79786899  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815697 -3.76957603 -2.75703013 -0.28191791 -1.        ]\n",
      "state value of state s1 is -3.8757300253606966\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815702 -3.76957603 -2.75703013 -0.28191791 -1.        ]\n",
      "state value of state s2 is -3.077306747126394\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815702 -3.76957607 -2.75703013 -0.28191791 -1.        ]\n",
      "state value of state s3 is -1.9522557429981398\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815702  -3.76957607  -2.75703017  -0.28191791  10.        ]\n",
      "state value of state s4 is 0.7978689626083111\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815702 -2.76957607 -1.75703017  0.71808207  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573003 -3.07730675 -1.95225574  0.79786896  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815702 -3.76957607 -2.75703017 -0.28191793 -1.        ]\n",
      "state value of state s1 is -3.8757300716903336\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815706 -3.76957607 -2.75703017 -0.28191793 -1.        ]\n",
      "state value of state s2 is -3.077306785750333\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815706 -3.76957611 -2.75703017 -0.28191793 -1.        ]\n",
      "state value of state s3 is -1.9522557751311667\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815706  -3.76957611  -2.7570302   -0.28191793  10.        ]\n",
      "state value of state s4 is 0.7978689403885326\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815706 -2.76957611 -1.7570302   0.71808205  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573007 -3.07730679 -1.95225578  0.79786894  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815706 -3.76957611 -2.7570302  -0.28191795 -1.        ]\n",
      "state value of state s1 is -3.8757301084589875\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4881571  -3.76957611 -2.7570302  -0.28191795 -1.        ]\n",
      "state value of state s2 is -3.0773068164035022\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4881571  -3.76957613 -2.7570302  -0.28191795 -1.        ]\n",
      "state value of state s3 is -1.952255800632945\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4881571   -3.76957613  -2.75703022  -0.28191795  10.        ]\n",
      "state value of state s4 is 0.7978689227542193\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4881571  -2.76957613 -1.75703022  0.71808203  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573011 -3.07730682 -1.9522558   0.79786892  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4881571  -3.76957613 -2.75703022 -0.28191797 -1.        ]\n",
      "state value of state s1 is -3.875730137639745\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815712 -3.76957613 -2.75703022 -0.28191797 -1.        ]\n",
      "state value of state s2 is -3.0773068407308193\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815712 -3.76957616 -2.75703022 -0.28191797 -1.        ]\n",
      "state value of state s3 is -1.9522558208719567\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815712  -3.76957616  -2.75703024  -0.28191797  10.        ]\n",
      "state value of state s4 is 0.7978689087590747\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815712 -2.76957616 -1.75703024  0.71808202  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573014 -3.07730684 -1.95225582  0.79786891  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815712 -3.76957616 -2.75703024 -0.28191798 -1.        ]\n",
      "state value of state s1 is -3.8757301607985095\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815714 -3.76957616 -2.75703024 -0.28191798 -1.        ]\n",
      "state value of state s2 is -3.077306860037741\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815714 -3.76957617 -2.75703024 -0.28191798 -1.        ]\n",
      "state value of state s3 is -1.9522558369342713\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815714  -3.76957617  -2.75703025  -0.28191798  10.        ]\n",
      "state value of state s4 is 0.7978688976520889\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815714 -2.76957617 -1.75703025  0.71808201  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573016 -3.07730686 -1.95225584  0.7978689   0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815714 -3.76957617 -2.75703025 -0.28191799 -1.        ]\n",
      "state value of state s1 is -3.875730179178032\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815716 -3.76957617 -2.75703025 -0.28191799 -1.        ]\n",
      "state value of state s2 is -3.0773068753603208\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815716 -3.76957619 -2.75703025 -0.28191799 -1.        ]\n",
      "state value of state s3 is -1.9522558496818283\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815716  -3.76957619  -2.75703026  -0.28191799  10.        ]\n",
      "state value of state s4 is 0.7978688888372365\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815716 -2.76957619 -1.75703026  0.718082    0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573018 -3.07730688 -1.95225585  0.79786889  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815716 -3.76957619 -2.75703026 -0.281918   -1.        ]\n",
      "state value of state s1 is -3.875730193764598\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815717 -3.76957619 -2.75703026 -0.281918   -1.        ]\n",
      "state value of state s2 is -3.0773068875208005\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815717 -3.7695762  -2.75703026 -0.281918   -1.        ]\n",
      "state value of state s3 is -1.9522558597986897\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815717  -3.7695762   -2.75703027  -0.281918    10.        ]\n",
      "state value of state s4 is 0.7978688818414927\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815717 -2.7695762  -1.75703027  0.71808199  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573019 -3.07730689 -1.95225586  0.79786888  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815717 -3.7695762  -2.75703027 -0.28191801 -1.        ]\n",
      "state value of state s1 is -3.875730205340955\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815718 -3.7695762  -2.75703027 -0.28191801 -1.        ]\n",
      "state value of state s2 is -3.077306897171739\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815718 -3.76957621 -2.75703027 -0.28191801 -1.        ]\n",
      "state value of state s3 is -1.952255867827748\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815718  -3.76957621  -2.75703028  -0.28191801  10.        ]\n",
      "state value of state s4 is 0.7978688762894512\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815718 -2.76957621 -1.75703028  0.71808199  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573021 -3.0773069  -1.95225587  0.79786888  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815718 -3.76957621 -2.75703028 -0.28191801 -1.        ]\n",
      "state value of state s1 is -3.875730214528314\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815719 -3.76957621 -2.75703028 -0.28191801 -1.        ]\n",
      "state value of state s2 is -3.0773069048310266\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815719 -3.76957621 -2.75703028 -0.28191801 -1.        ]\n",
      "state value of state s3 is -1.952255874199861\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815719  -3.76957621  -2.75703029  -0.28191801  10.        ]\n",
      "state value of state s4 is 0.7978688718831768\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815719 -2.76957621 -1.75703029  0.71808198  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573021 -3.0773069  -1.95225587  0.79786887  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815719 -3.76957621 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s1 is -3.875730221819691\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4881572  -3.76957621 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s2 is -3.077306910909678\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4881572  -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s3 is -1.9522558792569698\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4881572   -3.76957622  -2.75703029  -0.28191802  10.        ]\n",
      "state value of state s4 is 0.7978688683862192\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4881572  -2.76957622 -1.75703029  0.71808198  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573022 -3.07730691 -1.95225588  0.79786887  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4881572  -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s1 is -3.8757302276063568\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.4881572  -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s2 is -3.077306915733886\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.4881572  -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s3 is -1.95225588327045\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.4881572   -3.76957622  -2.75703029  -0.28191802  10.        ]\n",
      "state value of state s4 is 0.7978688656109238\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.4881572  -2.76957622 -1.75703029  0.71808198  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573023 -3.07730692 -1.95225588  0.79786887  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.4881572  -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s1 is -3.875730232198836\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815721 -3.76957622 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s2 is -3.0773069195625293\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.75703029 -0.28191802 -1.        ]\n",
      "state value of state s3 is -1.9522558864556736\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815721  -3.76957623  -2.7570303   -0.28191802  10.        ]\n",
      "state value of state s4 is 0.7978688634083622\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815721 -2.76957623 -1.7570303   0.71808198  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573023 -3.07730692 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s1 is -3.8757302358435717\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s2 is -3.07730692260106\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s3 is -1.9522558889835668\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815721  -3.76957623  -2.7570303   -0.28191802  10.        ]\n",
      "state value of state s4 is 0.7978688616603404\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815721 -2.76957623 -1.7570303   0.71808198  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573024 -3.07730692 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s1 is -3.875730238736148\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s2 is -3.0773069250125333\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191802 -1.        ]\n",
      "state value of state s3 is -1.9522558909897827\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815721  -3.76957623  -2.7570303   -0.28191802  10.        ]\n",
      "state value of state s4 is 0.7978688602730555\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815721 -2.76957623 -1.7570303   0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573024 -3.07730693 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815721 -3.76957623 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302410317878\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957623 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306926926355\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957623 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558925819784\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957623  -2.7570303   -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688591720624\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957623 -1.7570303   0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573024 -3.07730693 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957623 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302428536797\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957623 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069284452235\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.952255893845595\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.7570303   -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688582982798\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.7570303   0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573024 -3.07730693 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302442995893\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306929650645\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.7570303  -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558948484408\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688576048185\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573024 -3.07730693 -1.95225589  0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302454471096\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306930607306\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558956443306\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688570544659\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302463578176\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306931366542\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558962759737\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688566176888\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730247080584\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069319690953\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558967772654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688562710488\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302476541935\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306932447301\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558971751063\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688559959441\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730248109429\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.07730693282682\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558974908457\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688557776126\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730248470718\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306933128018\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558977414264\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688556043378\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302487574483\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306933367058\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558979402949\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688554668216\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730248985006\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306933556768\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558980981228\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688553576843\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730249165603\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069337073276\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558982233806\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688552710695\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302493089307\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.077306933826816\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558983227887\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688552023296\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302494226797\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069339216463\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558984016822\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688551477753\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.875730249512955\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069339969066\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558984642946\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.797868855104479\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.8757302495846\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069340566352\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558985139857\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688550701181\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n",
      "policy transitions are [ 0.5   0.25  0.25  0.    0.  ] for state s1\n",
      "probabilities [ 0.5   0.25  0.25  0.    0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s1 is -3.87573024964146\n",
      "policy transitions are [ 0.25  0.5   0.    0.25  0.  ] for state s2\n",
      "probabilities [ 0.25  0.5   0.    0.25  0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s2 is -3.0773069341040378\n",
      "policy transitions are [ 0.25  0.    0.25  0.5   0.  ] for state s3\n",
      "probabilities [ 0.25  0.    0.25  0.5   0.  ]\n",
      "bellman equations [-4.48815722 -3.76957624 -2.75703031 -0.28191803 -1.        ]\n",
      "state value of state s3 is -1.9522558985534222\n",
      "policy transitions are [ 0.    0.25  0.25  0.25  0.25] for state s4\n",
      "probabilities [ 0.    0.25  0.25  0.25  0.25]\n",
      "bellman equations [ -4.48815722  -3.76957624  -2.75703031  -0.28191803  10.        ]\n",
      "state value of state s4 is 0.7978688550428481\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-3.48815722 -2.76957624 -1.75703031  0.71808197  0.        ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n"
     ]
    }
   ],
   "source": [
    "random_policy_approx = Dynamic_Programmer(state_space, \n",
    "                        action_space, \n",
    "                        state_transitions,\n",
    "                        reward_functions,\n",
    "                        random_policy,\n",
    "                        discount=0.9,\n",
    "                        verbose=0)\n",
    "BACKUPS = 100\n",
    "for backup in range(BACKUPS):\n",
    "    value_function = random_policy_approx.backup_all_states()\n",
    "    print('value function is {}'.format(value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function is [-3.87573025 -3.07730693 -1.9522559   0.79786886  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('value function is {}'.format(value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  we can now try to evaluate an optimal policy\n",
    "#  for this simple MDP we can hard code the optimal policy\n",
    "\n",
    "def optimal_policy(state, action_space):\n",
    "    \n",
    "    optimal_action = {'s1' : 'right',\n",
    "                      's2' : 'down',\n",
    "                      's3' : 'right',\n",
    "                      's4' : 'right',\n",
    "                      's5' : 'right'}\n",
    "\n",
    "    action = optimal_action[state]\n",
    "\n",
    "    p_distribution = np.full(len(action_space), 0)\n",
    "    act_idx = np.argwhere(action == action_space)\n",
    "    p_distribution[act_idx] = 1\n",
    "    return action, p_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [-1. -1. -1. -1. -1.]\n",
      "state value of state s1 is -1.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [-1.9 -1.  -1.  -1.  -1. ]\n",
      "state value of state s2 is -1.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [-1.9 -1.9 -1.  -1.  -1. ]\n",
      "state value of state s3 is -1.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ -1.9  -1.9  -1.9  -1.   10. ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-0.9 -0.9 -0.9  9.   0. ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [ -1.  -1.  -1.  10.   0.]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [-1.9 -1.9 -1.9  8.  -1. ]\n",
      "state value of state s1 is -1.9\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [-2.71 -1.9  -1.9   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [-2.71  6.2  -1.9   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ -2.71   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [-1.71  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [ -1.9   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [-2.71  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "policy transitions are [ 0.  1.  0.  0.  0.] for state s1\n",
      "probabilities [ 0.  1.  0.  0.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s1 is 6.2\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s2\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s2 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  1.  0.] for state s3\n",
      "probabilities [ 0.  0.  0.  1.  0.]\n",
      "bellman equations [ 4.58  6.2   6.2   8.   -1.  ]\n",
      "state value of state s3 is 8.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s4\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [  4.58   6.2    6.2    8.    10.  ]\n",
      "state value of state s4 is 10.0\n",
      "policy transitions are [ 0.  0.  0.  0.  1.] for state s5\n",
      "probabilities [ 0.  0.  0.  0.  1.]\n",
      "bellman equations [ 5.58  7.2   7.2   9.    0.  ]\n",
      "state value of state s5 is 0.0\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_approx = Dynamic_Programmer(state_space, \n",
    "                        action_space, \n",
    "                        state_transitions,\n",
    "                        reward_functions,\n",
    "                        optimal_policy,\n",
    "                        discount=0.9,\n",
    "                        verbose=0)\n",
    "\n",
    "BACKUPS = 10\n",
    "for backup in range(BACKUPS):\n",
    "    optimal_value_function = optimal_policy_approx.backup_all_states()\n",
    "    print('value function is {}'.format(optimal_value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
