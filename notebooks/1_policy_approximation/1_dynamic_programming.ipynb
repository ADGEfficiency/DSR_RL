{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to demonstrate dynamic programming as a method for policy approximation.\n",
    "\n",
    "Aim is for the student to understand\n",
    "- concept of what we need to define a Markov Decision Process (state transitons, reward transitions)\n",
    "- how we use the Bellman equation to bootstrap our value function approximation\n",
    "\n",
    "- how dynamic programming can learn value functions without taking actions]\n",
    "- how dynamic programming needs the environment model to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = np.array(['s{}'.format(state) for state in np.arange(1,6)])\n",
    "action_space = np.array(['left', 'right', 'up', 'down'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  loading state transition probabilities from csvs\n",
    "state_transitions = {state:np.genfromtxt('{}.csv'.format(state), delimiter=',') for state in state_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  here for example are the state transition probabilities\n",
    "#  note that rows = actions, columns = states\n",
    "\n",
    "#  here are the probabilities for state s1\n",
    "#  the probability of action 'right' taking us to 's2' is 1\n",
    "#  the probability of action 'up' taking us to 's1' is 1\n",
    "#  i.e. P(s'|s,a)\n",
    "state_transitions['s1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  we can now do our reward transition probabilities\n",
    "reward_functions = {state:np.full((len(state_space)),-1) for state in state_space}\n",
    "\n",
    "#  these work in a similar way to our state transitions, except they are not conditioned upon actions\n",
    "#  only conditions upon state, next_state\n",
    "#  i.e R(s, s')\n",
    "#  this is OK for our problem - a more formal problem would have R(s, a, s')\n",
    "\n",
    "#  reward of all state->next_state transitons is -1\n",
    "reward_functions['s4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  now we manually fill in the rewards for our special states\n",
    "reward_functions['s4'][4] = 10\n",
    "reward_functions['s4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  's5' is a special absorbing state with no rewards and transition prob of 1 to itself\n",
    "reward_functions['s5'] = np.full((len(state_space)),0)\n",
    "reward_functions['s5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we need a policy to approximate - lets use a random policy\n",
    "\n",
    "def random_policy(state, action_space):\n",
    "    action = np.random.choice(action_space)\n",
    "    p_distribution = np.full(len(action_space),  1/len(action_space))\n",
    "    return action, p_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_Programmer(object):\n",
    "    def __init__(self, \n",
    "                 state_space, \n",
    "                 action_space, \n",
    "                 state_transitions, \n",
    "                 reward_functions,\n",
    "                 policy,\n",
    "                 discount,\n",
    "                 verbose=1):    \n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.state_transitions = state_transitions\n",
    "        self.reward_functions = reward_functions\n",
    "        self.policy = policy\n",
    "        self.discount = discount\n",
    "        self.verbose = verbose\n",
    "    \n",
    "        self.v_table = np.full(len(state_space),0.0)\n",
    "\n",
    "    def policy_transitions(self, state):\n",
    "        \"\"\"\n",
    "        For a given state, returns a distribution over\n",
    "        next states (not over actions!) for a given policy\n",
    "        \"\"\"\n",
    "        state_idx = np.argwhere(state==self.state_space).flatten()\n",
    "        #  our policy gives us a distribution over actions\n",
    "        _, action_p_dist = self.policy(state, self.action_space)\n",
    "        \n",
    "        #  we can now use our perfect environment model to \n",
    "        #  develop the state transition probabilities\n",
    "        \n",
    "        for next_state in self.state_space:    \n",
    "            all_probs = np.full(len(state_space),0)\n",
    "            all_rewards = np.full(len(state_space),0)\n",
    "            for act_idx, act_prob in enumerate(action_p_dist):   \n",
    "                \n",
    "                state_probs = self.state_transitions[state][act_idx].flatten()\n",
    "                probs = act_prob * state_probs\n",
    "                all_probs = np.add(all_probs, probs)\n",
    "                \n",
    "        return all_probs\n",
    "    \n",
    "    def backup_state(self, state, probabilities): \n",
    "        rewards = self.reward_functions[state]\n",
    "        discounted_values = self.discount * self.v_table  # note the recursive use of v.table!\n",
    "        bellman_equations = np.add(rewards, discounted_values)\n",
    "        \n",
    "        state_value = np.sum(np.multiply(probabilities, bellman_equations))\n",
    "        \n",
    "        state_idx = np.argwhere(state==self.state_space).flatten()\n",
    "        self.v_table[state_idx] = state_value\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print('backing up state {}'.format(state))\n",
    "            print('probabilities are {}'.format(probabilities))\n",
    "            print('rewards are {}'.format(rewards))\n",
    "            print('discounted_values are {}'.format(discounted_values))  \n",
    "            print('bellman equations are {}'.format(bellman_equations))\n",
    "            print('state value is {}'.format(state_value))\n",
    "            print('value function table {}'.format(self.v_table))\n",
    "        return state_value\n",
    "    \n",
    "    def backup_all_states(self):\n",
    "        \n",
    "        for state in self.state_space:\n",
    "            probabilities = self.policy_transitions(state)\n",
    "            state_value = self.backup_state(state, probabilities)\n",
    "        return self.v_table  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function is [-1.      -1.225   -1.225    1.19875  0.     ]\n",
      "value function is [-2.00125    -1.7318125  -1.18646875  1.36310547  0.        ]\n",
      "value function is [-2.55717578 -2.04798145 -1.22892256  1.31939533  0.        ]\n",
      "value function is [-2.8880325  -2.27453501 -1.33258699  1.2352615   0.        ]\n",
      "value function is [-3.11121708 -2.44563076 -1.44398824  1.15276956  0.        ]\n",
      "value function is [-3.27521196 -2.57808338 -1.54307374  1.0821128   0.        ]\n",
      "value function is [-3.40110574 -2.68191093 -1.62548962  1.02431025  0.        ]\n",
      "value function is [-3.49966271 -2.76381422 -1.69221966  0.97786218  0.        ]\n",
      "value function is [-3.57745584 -2.82862497 -1.745639    0.9408096   0.        ]\n",
      "value function is [-3.63906452 -2.8799886  -1.78819398  0.91134108  0.        ]\n",
      "value function is [-3.68792011 -2.92072515 -1.82202218  0.88793359  0.        ]\n",
      "value function is [-3.7266822  -2.95304475 -1.84888837  0.86935011  0.        ]\n",
      "value function is [-3.75744194 -2.9786908  -1.87021677  0.85459957  0.        ]\n",
      "value function is [-3.78185308 -2.9990429  -1.88714591  0.84289242  0.        ]\n",
      "value function is [-3.80122637 -3.01519444 -1.90058217  0.83360106  0.        ]\n",
      "value function is [-3.8166016  -3.02801262 -1.91124588  0.82622708  0.        ]\n",
      "value function is [-3.82880388 -3.03818546 -1.91970901  0.82037484  0.        ]\n",
      "value function is [-3.838488   -3.04625892 -1.92642565  0.81573031  0.        ]\n",
      "value function is [-3.84617363 -3.05266626 -1.9317562   0.81204427  0.        ]\n",
      "value function is [-3.85227319 -3.05775133 -1.93598669  0.80911891  0.        ]\n",
      "value function is [-3.85711399 -3.06178699 -1.93934415  0.80679725  0.        ]\n",
      "value function is [-3.8609558  -3.06498982 -1.94200873  0.80495471  0.        ]\n",
      "value function is [-3.86400478 -3.06753169 -1.94412342  0.80349241  0.        ]\n",
      "value function is [-3.86642455 -3.06954899 -1.94580171  0.80233188  0.        ]\n",
      "value function is [-3.86834496 -3.07114999 -1.94713365  0.80141086  0.        ]\n",
      "value function is [-3.86986905 -3.07242059 -1.94819072  0.8006799   0.        ]\n",
      "value function is [-3.87107862 -3.07342898 -1.94902965  0.80009979  0.        ]\n",
      "value function is [-3.87203857 -3.07422926 -1.94969544  0.79963939  0.        ]\n",
      "value function is [-3.87280042 -3.0748644  -1.95022384  0.79927401  0.        ]\n",
      "value function is [-3.87340504 -3.07536846 -1.95064319  0.79898403  0.        ]\n",
      "value function is [-3.87388489 -3.0757685  -1.95097601  0.79875389  0.        ]\n",
      "value function is [-3.87426572 -3.07608599 -1.95124014  0.79857125  0.        ]\n",
      "value function is [-3.87456795 -3.07633795 -1.95144976  0.7984263   0.        ]\n",
      "value function is [-3.87480781 -3.07653792 -1.95161612  0.79831126  0.        ]\n",
      "value function is [-3.87499817 -3.07669662 -1.95174815  0.79821996  0.        ]\n",
      "value function is [-3.87514925 -3.07682257 -1.95185293  0.7981475   0.        ]\n",
      "value function is [-3.87526915 -3.07692253 -1.95193609  0.79809     0.        ]\n",
      "value function is [-3.87536431 -3.07700186 -1.95200209  0.79804436  0.        ]\n",
      "value function is [-3.87543983 -3.07706482 -1.95205447  0.79800814  0.        ]\n",
      "value function is [-3.87549976 -3.07711478 -1.95209604  0.7979794   0.        ]\n",
      "value function is [-3.87554733 -3.07715444 -1.95212903  0.79795659  0.        ]\n",
      "value function is [-3.87558508 -3.07718591 -1.95215521  0.79793848  0.        ]\n",
      "value function is [-3.87561504 -3.07721088 -1.95217599  0.79792411  0.        ]\n",
      "value function is [-3.87563881 -3.0772307  -1.95219248  0.79791271  0.        ]\n",
      "value function is [-3.87565768 -3.07724644 -1.95220557  0.79790366  0.        ]\n",
      "value function is [-3.87567266 -3.07725892 -1.95221595  0.79789648  0.        ]\n",
      "value function is [-3.87568454 -3.07726883 -1.9522242   0.79789078  0.        ]\n",
      "value function is [-3.87569398 -3.07727669 -1.95223074  0.79788625  0.        ]\n",
      "value function is [-3.87570146 -3.07728293 -1.95223593  0.79788266  0.        ]\n",
      "value function is [-3.8757074  -3.07728789 -1.95224005  0.79787981  0.        ]\n",
      "value function is [-3.87571212 -3.07729182 -1.95224332  0.79787755  0.        ]\n",
      "value function is [-3.87571586 -3.07729494 -1.95224592  0.79787576  0.        ]\n",
      "value function is [-3.87571883 -3.07729741 -1.95224798  0.79787433  0.        ]\n",
      "value function is [-3.87572119 -3.07729938 -1.95224961  0.7978732   0.        ]\n",
      "value function is [-3.87572306 -3.07730094 -1.95225091  0.7978723   0.        ]\n",
      "value function is [-3.87572454 -3.07730217 -1.95225194  0.79787159  0.        ]\n",
      "value function is [-3.87572572 -3.07730316 -1.95225276  0.79787103  0.        ]\n",
      "value function is [-3.87572665 -3.07730394 -1.9522534   0.79787058  0.        ]\n",
      "value function is [-3.8757274  -3.07730456 -1.95225392  0.79787022  0.        ]\n",
      "value function is [-3.87572799 -3.07730505 -1.95225433  0.79786994  0.        ]\n"
     ]
    }
   ],
   "source": [
    "random_policy_approx = Dynamic_Programmer(state_space, \n",
    "                        action_space, \n",
    "                        state_transitions,\n",
    "                        reward_functions,\n",
    "                        random_policy,\n",
    "                        discount=0.9,\n",
    "                        verbose=0)\n",
    "BACKUPS = 60\n",
    "for backup in range(BACKUPS):\n",
    "    value_function = random_policy_approx.backup_all_states()\n",
    "    print('value function is {}'.format(value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can now try to evaluate an optimal policy\n",
    "#  for this simple MDP we can hard code the optimal policy\n",
    "\n",
    "def optimal_policy(state, action_space):\n",
    "    \n",
    "    optimal_action = {'s1' : 'right',\n",
    "                      's2' : 'down',\n",
    "                      's3' : 'right',\n",
    "                      's4' : 'right',\n",
    "                      's5' : 'right'}\n",
    "\n",
    "    action = optimal_action[state]\n",
    "\n",
    "    p_distribution = np.full(len(action_space), 0)\n",
    "    act_idx = np.argwhere(action == action_space)\n",
    "    p_distribution[act_idx] = 1\n",
    "    return action, p_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function is [ -1.  -1.  -1.  10.   0.]\n",
      "value function is [ -1.9   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n",
      "value function is [  6.2   8.    8.   10.    0. ]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_approx = Dynamic_Programmer(state_space, \n",
    "                        action_space, \n",
    "                        state_transitions,\n",
    "                        reward_functions,\n",
    "                        optimal_policy,\n",
    "                        discount=0.9,\n",
    "                        verbose=0)\n",
    "\n",
    "BACKUPS = 10\n",
    "for backup in range(BACKUPS):\n",
    "    optimal_value_function = optimal_policy_approx.backup_all_states()\n",
    "    print('value function is {}'.format(optimal_value_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
