### Recap

Three sources of generalization error
- ?
- ?
- ?

Missing relevant patterns in data = ?

Seeing patterns that aren't there = ? 

One advantage & disadvantage of lookup tables
- advantage = ?
- disadvantage = ? 

iid = ? and ? distributed 

Larger batches -> ? learning rate

Why do we pass in `None` for the first dimension in TensorFlow
`tf.placeholder(shape=(None, 14, 2))`

### Recap answers

Three sources of generalization error
- bias
- variance
- noise

Missing relevant patterns in data = bias

Seeing patterns that aren't there = variance 

One advantage & disadvantage of lookup tables
- advantage = stability
- disadvantage = no aliasing between states, curse of dimensionality

iid = independent and identically distributed 

Larger batches -> larger learning rate
- better estimation of the gradient

Why do we pass in `None` for the first dimension in TensorFlow
`tf.placeholder(shape=(None, 14, 2))`

- first dimension is the batch dimension




---
### Recap

How does reinforcement learning break iid?
- ? 
- ? 

What is the credit assignment problem?
- ? 

An MDP is composed of two objects & three signals - what are they?
- ? 
- ? 

What is off-policy learning?
- ?

Why do we discount future rewards?
- ?
- ?

---
### Recap

How does reinforcement learning break iid?
- we don't sample experience independently - sampling biased by the agent & environment
- our experience is not independent - based on trajectory in the MDP

What is the credit assignment problem?
- working out which action gave us which rewards

An MDP is composed of two objects & three signals - what are they?
- agent & environment
- state, action, reward

What is off-policy learning?
- learning from experience generated by other policies

Why do we discount future rewards?
- makes return a geometric series
- discounting is common in decision making

---
### Recap 

What does a value function predict?

What is general policy iteration?

How does the Bellman Equation help us learn?

For each of the three approximation methods (DP, MC, TD)
- needs an environment model?
- bootstrapped?
- 

If we wanted to control using $V(s)$, what else would we need?

What makes Q-Learning off-policy?

What is the deadly triad?

What is the benefit of experience replay?

What is the benefit of a target network?

---
### Recap 

What does a value function predict?
- future expected discounted reward

What is general policy iteration?
- process of letting policy evaluation and policy improvement interact 

How does the Bellman Equation help us learn?
- allows us to create bootstrapped targets

If we wanted to control using $V(s)$, what else would we need?
- the state transition function

What makes Q-Learning off-policy?
- maximization over all possible next actions

What is the deadly triad?
- function approximation
- off-policy learning
- bootstrapping

What is the benefit of experience replay?
- decorrelating experience
- data efficiency

What is the benefit of a target network?
- stability
- decorrelating the target used to train the network from the network output
---
### Recap

Elgibility traces allow us to trade bias and variance

Elgibility traces assign the temporal difference to different states

What two problems does prioritized experience replay introduce?
- lack of diversity, solved by making sampling stochastic
- introduces bias, solved using importance sampling

What problem does DDQN address?
- maximization bias

---
### Recap

Motivations for policy gradients
- optimize what we care about directly
- high dimensional spaces

How do we parameterize a continuous action
- output of network is the mean & variance of a Gaussian

What does the log-likelihood trick allow us to do
- get the gradient of an unknown function

Two intuitions behind the score function
- make probable actions more probable
- make high return actions more probable

What is the motivation behind asynchronous learning in A3C?
- decorrelating experience

---
### Recap

How does AlphaGo reduce search width
- policy network to focus on high probability states

How does AlphaGo reduce search depth
- a value network

MCTS is a planning algorithm - what does AlphaGo use for an environment model?
- the linear fast rollout policy + game rules

Innovations in AlphaGo Zero
- combining the policy and value network
- using MCTS during acting to create targets to learn from


---

---
### Recap

Quick experiments on small test problems
- make learning easy 
- automate experiments

Visualize the learning process

Multiple random seeds

Preprocess/scale observations/targets etc

Deep RL is hard and sample inefficient
